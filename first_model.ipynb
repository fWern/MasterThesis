{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e7ff9891-f8b6-42e9-8183-ae390474399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import log_loss\n",
    "import clip\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3a20b6ca-25bb-475e-acb6-c042afa71add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d9bf3333-4c1c-4492-938e-173a8e8fe45e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib._GeneratorContextManager at 0x25308ff9350>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.cuda.sdp_kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "013a36ef-4a61-4cff-b45c-d4c6b391a128",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.enable_flash_sdp(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e6f84ad8-0477-45b4-be9c-977b4a5ecdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths \n",
    "# hearthstone\n",
    "DATA_PATH_HEARTHSTONE_RACE = '../dataset/Hearthstone-Minion-race/' #label: race\n",
    "DATA_PATH_HEARTHSTONE_CARDCLASS = '../dataset/Hearthstone-All-cardClass/' # label: card class\n",
    "DATA_PATH_HEARTHSTONE_ALLSET = '../dataset/Hearthstone-All-set/' # label: set\n",
    "DATA_PATH_HEARTHSTONE_SPELLSCHOOL = '../dataset/Hearthstone-Spell-spellSchool/' # label: set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "89693f58-abfb-4c84-9a01-51affab8f8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8992900-38ec-496d-9d01-5b42cc656729",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8e92854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df, id):\n",
    "    df['text'] = df['text'].fillna('unknown')\n",
    "    df['text'] = df['text'].apply(preprocess_sentence)\n",
    "    df['artist'] = df['artist'].fillna('unknown')\n",
    "    df['mechanics'] = df['mechanics'].fillna('unknown')\n",
    "    df['mechanics'] = df['mechanics'].str.strip(\"[]''\")\n",
    "    if id:\n",
    "        df['combined_text'] = df['name'].str.lower() + ' ' + df['id'].str.lower()  + ' ' + df['artist'].str.lower()  + ' ' + df['text'].str.lower()  + ' ' + df['mechanics'].str.lower() \n",
    "    else:\n",
    "        df['combined_text'] = df['name'].str.lower() + ' ' + df['artist'].str.lower()  + ' ' + df['text'].str.lower()  + ' ' + df['mechanics'].str.lower() \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "284fbd91-88e0-4829-aac3-3e35498e8481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = re.sub(r'<[^>]+>', '', sentence)\n",
    "    sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "    sentence = sentence.replace('\\n', ' ')\n",
    "    sentence = sentence.lower()\n",
    "    sentence = ' '.join(sentence.split())\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "35e6992a-6cc8-48c6-9d20-ebcb7cad5bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_image_text(texts, images, clip_model, clip_preprocess):\n",
    "    image_text = []\n",
    "    label_list = []\n",
    "    for idx in tqdm(range(len(texts))):\n",
    "        text = texts[idx]\n",
    "        image = images[idx]\n",
    "        text = clip.tokenize(text).to(device)\n",
    "        image = clip_preprocess(Image.open(image)).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            text_features = clip_model.encode_text(text)\n",
    "            image_features = clip_model.encode_image(image)\n",
    "        combined_features = torch.cat((text_features, image_features), 1)\n",
    "        image_text.append(combined_features)\n",
    "    return image_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "306b9e45-19f5-44b4-8ea6-3ed9777fa16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(df, data_path, clip_model, clip_preprocess, id):\n",
    "    df = preprocess_df(df, id)\n",
    "    texts = list(df['combined_text'])\n",
    "    images = [data_path + img for img in list(df['Image Path'])]\n",
    "    data = combine_image_text(texts, images, clip_model, clip_preprocess)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d644e892-6507-43fb-a983-69ecb66b78de",
   "metadata": {},
   "source": [
    "# Model Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ad92820f-16f1-46dd-aa43-c7296da5f0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features[idx].clone().detach().to(torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d34832e9-eba6-44ed-8b14-a6d6d1992c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc_1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc_2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc_3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1) \n",
    "        out = self.dropout(self.relu(self.fc_1(x)))\n",
    "        out = self.dropout(self.relu(self.fc_2(out)))\n",
    "        out = self.fc_3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "053cf1e8-e3d8-4f98-b1bb-be753364de6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, dev_loader):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        model.train()\n",
    "        for feature, label in train_loader:\n",
    "            feature = feature.to(device)\n",
    "            label = label.to(device)\n",
    "            output = model(feature)\n",
    "            loss = criterion(output, label)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * feature.size(0)\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for dev_feature, dev_labels in dev_loader:\n",
    "                dev_feature = dev_feature.to(device)\n",
    "                dev_labels = dev_labels.to(device)\n",
    "                output = model(dev_feature)\n",
    "                loss = criterion(output, dev_labels)\n",
    "                val_loss += loss.item() * dev_feature.size(0)\n",
    "                    \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        val_loss /= len(dev_loader.dataset)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f} - Validation Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "01998613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, test_loader):\n",
    "    total_accuracy = 0.0\n",
    "    total_samples = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for feature, label in test_loader:\n",
    "            feature = feature.to(device)\n",
    "            label = label.to(device)\n",
    "            output = model(feature)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            correct = (predicted == label).sum().item()\n",
    "            total_accuracy += correct\n",
    "            total_samples += label.size(0)\n",
    "    accuracy = total_accuracy / total_samples\n",
    "    print(f\"Accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "92972632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss: 1.789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felix\\anaconda3\\envs\\masterthesis\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2969: UserWarning: The y_pred values do not sum to one. Starting from 1.5 thiswill result in an error.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_log_loss(model, test_loader):\n",
    "    all_predicted_probabilities = []\n",
    "    all_labels = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for feature, label in test_loader:\n",
    "            feature = feature.to(device)\n",
    "            label = label.to(device)\n",
    "            output = model(feature)\n",
    "            probabilities = torch.nn.functional.softmax(output, dim=1)  # Calculate softmax probabilities\n",
    "            probabilities = probabilities.cpu().numpy()  # Convert to numpy array\n",
    "            probabilities = np.clip(probabilities, 1e-7, 1 - 1e-7)  # Clip probabilities to avoid extreme values\n",
    "            all_predicted_probabilities.extend(probabilities)\n",
    "            all_labels.extend(label.cpu().numpy())\n",
    "\n",
    "    # Compute overall log loss using sklearn's log_loss function\n",
    "    logloss = log_loss(all_labels, all_predicted_probabilities)\n",
    "    return logloss\n",
    "\n",
    "# Usage\n",
    "logloss = evaluate_log_loss(model, test_loader)\n",
    "print(f\"Log Loss: {logloss:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aced59a-21c7-4af7-b4be-8d58d94bb079",
   "metadata": {},
   "source": [
    "# Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ad2d3f61-d43a-42d9-9bb9-c44bd0e0b8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df_train = pd.read_csv(DATA_PATH_HEARTHSTONE_RACE + \"/train.csv\")\n",
    "df_test = pd.read_csv(DATA_PATH_HEARTHSTONE_RACE + \"/test.csv\")\n",
    "df_dev = pd.read_csv(DATA_PATH_HEARTHSTONE_RACE + \"/dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3e3ee0db-0a28-4109-8088-bd936517d01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a708c7d3-8d0c-4855-a577-76cf2cfd9208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5398/5398 [05:22<00:00, 16.76it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1012/1012 [01:03<00:00, 16.04it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 337/337 [00:20<00:00, 16.34it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = process_df(df_train, DATA_PATH_HEARTHSTONE_RACE, clip_model, clip_preprocess, True)\n",
    "train_labels = label_encoder.fit_transform(df_train['race'])\n",
    "test_data = process_df(df_test, DATA_PATH_HEARTHSTONE_RACE, clip_model, clip_preprocess, True)\n",
    "test_labels = label_encoder.fit_transform(df_test['race'])\n",
    "dev_data = process_df(df_dev, DATA_PATH_HEARTHSTONE_RACE, clip_model, clip_preprocess, True)\n",
    "dev_labels = label_encoder.fit_transform(df_dev['race'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8d9bdaaf-210c-408d-8bc6-ac0a9a49f3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialized model\n",
    "input_size = train_data[0].size(1)\n",
    "hidden_size = 128\n",
    "num_epochs = 20\n",
    "num_classes = len(label_encoder.classes_)\n",
    "lr = 0.0002\n",
    "batch_size = 16\n",
    "    \n",
    "model = Model(input_size, hidden_size, num_classes).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4da8a629-931a-48d1-9efb-53cf9bba2cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize data loader\n",
    "train_dataset = CustomDataset(train_data, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = CustomDataset(test_data, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "dev_dataset = CustomDataset(dev_data, dev_labels)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "266af29e-99ee-4c1a-bf50-b7a9ae2f9059",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20]\n",
      "  Train Loss: 1.7900 - Validation Loss: 1.4893\n",
      "Epoch [2/20]\n",
      "  Train Loss: 1.3783 - Validation Loss: 1.2656\n",
      "Epoch [3/20]\n",
      "  Train Loss: 1.1545 - Validation Loss: 1.0386\n",
      "Epoch [4/20]\n",
      "  Train Loss: 0.9819 - Validation Loss: 0.9156\n",
      "Epoch [5/20]\n",
      "  Train Loss: 0.8940 - Validation Loss: 0.8355\n",
      "Epoch [6/20]\n",
      "  Train Loss: 0.8062 - Validation Loss: 0.7836\n",
      "Epoch [7/20]\n",
      "  Train Loss: 0.7459 - Validation Loss: 0.7205\n",
      "Epoch [8/20]\n",
      "  Train Loss: 0.6734 - Validation Loss: 0.6944\n",
      "Epoch [9/20]\n",
      "  Train Loss: 0.6399 - Validation Loss: 0.6601\n",
      "Epoch [10/20]\n",
      "  Train Loss: 0.5845 - Validation Loss: 0.6347\n",
      "Epoch [11/20]\n",
      "  Train Loss: 0.5449 - Validation Loss: 0.6260\n",
      "Epoch [12/20]\n",
      "  Train Loss: 0.5123 - Validation Loss: 0.6247\n",
      "Epoch [13/20]\n",
      "  Train Loss: 0.4867 - Validation Loss: 0.5991\n",
      "Epoch [14/20]\n",
      "  Train Loss: 0.4607 - Validation Loss: 0.5862\n",
      "Epoch [15/20]\n",
      "  Train Loss: 0.4383 - Validation Loss: 0.5767\n",
      "Epoch [16/20]\n",
      "  Train Loss: 0.4038 - Validation Loss: 0.5746\n",
      "Epoch [17/20]\n",
      "  Train Loss: 0.4016 - Validation Loss: 0.5735\n",
      "Epoch [18/20]\n",
      "  Train Loss: 0.3658 - Validation Loss: 0.5655\n",
      "Epoch [19/20]\n",
      "  Train Loss: 0.3507 - Validation Loss: 0.5898\n",
      "Epoch [20/20]\n",
      "  Train Loss: 0.3313 - Validation Loss: 0.5551\n",
      "Accuracy: 0.853\n",
      "Log Loss: 0.460\n"
     ]
    }
   ],
   "source": [
    "# train model and evaluation\n",
    "train(model, train_loader, dev_loader)\n",
    "evaluate_accuracy(model, test_loader)\n",
    "evaluate_log_loss(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19685cb1-2fa3-4851-95f4-1a0e76b7e861",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_class_metrics(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0106e919-3ff4-4bb8-af2c-70f8314b33ea",
   "metadata": {},
   "source": [
    "# Card Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d854e671-0f05-4895-aebc-a9b985404d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(DATA_PATH_HEARTHSTONE_CARDCLASS + \"/train.csv\")\n",
    "df_test = pd.read_csv(DATA_PATH_HEARTHSTONE_CARDCLASS + \"/test.csv\")\n",
    "df_dev = pd.read_csv(DATA_PATH_HEARTHSTONE_CARDCLASS + \"/dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a063613c-9971-463a-9078-b807f2a20c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2daf29ab-3c14-49a3-8bfe-3f076e7cefc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 8561/8561 [08:51<00:00, 16.10it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1603/1603 [01:34<00:00, 16.96it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 536/536 [00:32<00:00, 16.51it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = process_df(df_train, DATA_PATH_HEARTHSTONE_CARDCLASS, clip_model, clip_preprocess, True)\n",
    "train_labels = label_encoder.fit_transform(df_train['cardClass'])\n",
    "test_data = process_df(df_test, DATA_PATH_HEARTHSTONE_CARDCLASS, clip_model, clip_preprocess, True)\n",
    "test_labels = label_encoder.fit_transform(df_test['cardClass'])\n",
    "dev_data = process_df(df_dev, DATA_PATH_HEARTHSTONE_CARDCLASS, clip_model, clip_preprocess, True)\n",
    "dev_labels = label_encoder.fit_transform(df_dev['cardClass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d785fd79-dffb-4a6c-b54a-c845c8592f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DEATHKNIGHT': 0,\n",
       " 'DEMONHUNTER': 1,\n",
       " 'DRUID': 2,\n",
       " 'HUNTER': 3,\n",
       " 'MAGE': 4,\n",
       " 'NEUTRAL': 5,\n",
       " 'NONE_cardClass': 6,\n",
       " 'PALADIN': 7,\n",
       " 'PRIEST': 8,\n",
       " 'ROGUE': 9,\n",
       " 'SHAMAN': 10,\n",
       " 'WARLOCK': 11,\n",
       " 'WARRIOR': 12}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "class_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "456606f0-1015-479e-819a-5bc55e5956e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialized model\n",
    "input_size = train_data[0].size(1)\n",
    "hidden_size = 128\n",
    "num_epochs = 20\n",
    "num_classes = len(label_encoder.classes_)\n",
    "lr = 0.0002\n",
    "batch_size = 16\n",
    "    \n",
    "model = Model(input_size, hidden_size, num_classes).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "facb88a4-2737-4cd6-9eed-4660d7c7e223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize data loader\n",
    "train_dataset = CustomDataset(train_data, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = CustomDataset(test_data, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "dev_dataset = CustomDataset(dev_data, dev_labels)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "75541eda-cb1c-4dd3-9360-aeb4b7fcbd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20]\n",
      "  Train Loss: 1.9485 - Validation Loss: 1.7714\n",
      "Epoch [2/20]\n",
      "  Train Loss: 1.7056 - Validation Loss: 1.5524\n",
      "Epoch [3/20]\n",
      "  Train Loss: 1.5532 - Validation Loss: 1.4228\n",
      "Epoch [4/20]\n",
      "  Train Loss: 1.4379 - Validation Loss: 1.3060\n",
      "Epoch [5/20]\n",
      "  Train Loss: 1.3421 - Validation Loss: 1.2318\n",
      "Epoch [6/20]\n",
      "  Train Loss: 1.2709 - Validation Loss: 1.1834\n",
      "Epoch [7/20]\n",
      "  Train Loss: 1.2020 - Validation Loss: 1.1418\n",
      "Epoch [8/20]\n",
      "  Train Loss: 1.1479 - Validation Loss: 1.1007\n",
      "Epoch [9/20]\n",
      "  Train Loss: 1.1073 - Validation Loss: 1.0693\n",
      "Epoch [10/20]\n",
      "  Train Loss: 1.0538 - Validation Loss: 1.0493\n",
      "Epoch [11/20]\n",
      "  Train Loss: 1.0113 - Validation Loss: 1.0362\n",
      "Epoch [12/20]\n",
      "  Train Loss: 0.9733 - Validation Loss: 1.0018\n",
      "Epoch [13/20]\n",
      "  Train Loss: 0.9300 - Validation Loss: 0.9919\n",
      "Epoch [14/20]\n",
      "  Train Loss: 0.8939 - Validation Loss: 0.9684\n",
      "Epoch [15/20]\n",
      "  Train Loss: 0.8541 - Validation Loss: 0.9799\n",
      "Epoch [16/20]\n",
      "  Train Loss: 0.8358 - Validation Loss: 0.9913\n",
      "Epoch [17/20]\n",
      "  Train Loss: 0.8070 - Validation Loss: 0.9536\n",
      "Epoch [18/20]\n",
      "  Train Loss: 0.7666 - Validation Loss: 0.9651\n",
      "Epoch [19/20]\n",
      "  Train Loss: 0.7476 - Validation Loss: 0.9061\n",
      "Epoch [20/20]\n",
      "  Train Loss: 0.7150 - Validation Loss: 0.9455\n",
      "Accuracy: 0.667\n",
      "Log Loss: 1.089\n"
     ]
    }
   ],
   "source": [
    "# train model and evaluation\n",
    "train(model, train_loader, dev_loader)\n",
    "evaluate_accuracy(model, test_loader)\n",
    "evaluate_log_loss(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c9bd26-a04e-4383-b97e-fd9f45edbb0d",
   "metadata": {},
   "source": [
    "# Spellschool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5b79d59a-dccf-4034-ba66-d6d49c1c886c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(DATA_PATH_HEARTHSTONE_SPELLSCHOOL + \"/train.csv\")\n",
    "df_test = pd.read_csv(DATA_PATH_HEARTHSTONE_SPELLSCHOOL + \"/test.csv\")\n",
    "df_dev = pd.read_csv(DATA_PATH_HEARTHSTONE_SPELLSCHOOL + \"/dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "217cd644-e509-4b3b-aca0-855037b7fc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6dd99155-8362-45ab-bd8a-4f507924f916",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2715/2715 [02:43<00:00, 16.65it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 508/508 [00:30<00:00, 16.70it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 170/170 [00:10<00:00, 16.39it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = process_df(df_train, DATA_PATH_HEARTHSTONE_SPELLSCHOOL, clip_model, clip_preprocess, True)\n",
    "train_labels = label_encoder.fit_transform(df_train['spellSchool'])\n",
    "test_data = process_df(df_test, DATA_PATH_HEARTHSTONE_SPELLSCHOOL, clip_model, clip_preprocess, True)\n",
    "test_labels = label_encoder.fit_transform(df_test['spellSchool'])\n",
    "dev_data = process_df(df_dev, DATA_PATH_HEARTHSTONE_SPELLSCHOOL, clip_model, clip_preprocess, True)\n",
    "dev_labels = label_encoder.fit_transform(df_dev['spellSchool'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1e81f902-8824-466f-9d50-7ffe68edfda6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ARCANE': 0,\n",
       " 'FEL': 1,\n",
       " 'FIRE': 2,\n",
       " 'FROST': 3,\n",
       " 'HOLY': 4,\n",
       " 'NATURE': 5,\n",
       " 'NONE_spellSchool': 6,\n",
       " 'SHADOW': 7}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "class_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ddb52b18-bb3c-4133-ad24-75c86f73e520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialized model\n",
    "input_size = train_data[0].size(1)\n",
    "hidden_size = 128\n",
    "num_epochs = 20\n",
    "num_classes = len(label_encoder.classes_)\n",
    "lr = 0.0002\n",
    "batch_size = 16\n",
    "    \n",
    "model = Model(input_size, hidden_size, num_classes).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "86448062-1f77-45d3-9cb6-75a531dee330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize data loader\n",
    "train_dataset = CustomDataset(train_data, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = CustomDataset(test_data, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "dev_dataset = CustomDataset(dev_data, dev_labels)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "93a33927-255d-4cbc-ab19-39afad9c301b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20]\n",
      "  Train Loss: 1.3241 - Validation Loss: 1.1467\n",
      "Epoch [2/20]\n",
      "  Train Loss: 1.1155 - Validation Loss: 1.0641\n",
      "Epoch [3/20]\n",
      "  Train Loss: 1.0132 - Validation Loss: 0.9729\n",
      "Epoch [4/20]\n",
      "  Train Loss: 0.9169 - Validation Loss: 0.8972\n",
      "Epoch [5/20]\n",
      "  Train Loss: 0.8298 - Validation Loss: 0.8168\n",
      "Epoch [6/20]\n",
      "  Train Loss: 0.7530 - Validation Loss: 0.8205\n",
      "Epoch [7/20]\n",
      "  Train Loss: 0.6885 - Validation Loss: 0.7211\n",
      "Epoch [8/20]\n",
      "  Train Loss: 0.6185 - Validation Loss: 0.6820\n",
      "Epoch [9/20]\n",
      "  Train Loss: 0.5871 - Validation Loss: 0.6805\n",
      "Epoch [10/20]\n",
      "  Train Loss: 0.5267 - Validation Loss: 0.6220\n",
      "Epoch [11/20]\n",
      "  Train Loss: 0.4939 - Validation Loss: 0.6016\n",
      "Epoch [12/20]\n",
      "  Train Loss: 0.4635 - Validation Loss: 0.5671\n",
      "Epoch [13/20]\n",
      "  Train Loss: 0.4389 - Validation Loss: 0.5519\n",
      "Epoch [14/20]\n",
      "  Train Loss: 0.4091 - Validation Loss: 0.5563\n",
      "Epoch [15/20]\n",
      "  Train Loss: 0.3669 - Validation Loss: 0.5244\n",
      "Epoch [16/20]\n",
      "  Train Loss: 0.3503 - Validation Loss: 0.5503\n",
      "Epoch [17/20]\n",
      "  Train Loss: 0.3324 - Validation Loss: 0.5291\n",
      "Epoch [18/20]\n",
      "  Train Loss: 0.3093 - Validation Loss: 0.5387\n",
      "Epoch [19/20]\n",
      "  Train Loss: 0.2841 - Validation Loss: 0.5218\n",
      "Epoch [20/20]\n",
      "  Train Loss: 0.2640 - Validation Loss: 0.5198\n",
      "Accuracy: 0.841\n",
      "Log Loss: 0.499\n"
     ]
    }
   ],
   "source": [
    "# train model and evaluation\n",
    "train(model, train_loader, dev_loader)\n",
    "evaluate_accuracy(model, test_loader)\n",
    "evaluate_log_loss(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9176e02-7505-4c79-9960-732af23c7219",
   "metadata": {},
   "source": [
    "# Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3c6416f5-444e-45a2-b52a-21b801722d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(DATA_PATH_HEARTHSTONE_ALLSET + \"/train.csv\")\n",
    "df_test = pd.read_csv(DATA_PATH_HEARTHSTONE_ALLSET + \"/test.csv\")\n",
    "df_dev = pd.read_csv(DATA_PATH_HEARTHSTONE_ALLSET + \"/dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "989d3aa9-d0da-4344-b5dd-ed1b9878ab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a54035ba-128a-4793-bb41-49d07d55cc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 8548/8548 [08:22<00:00, 17.00it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1603/1603 [01:33<00:00, 17.21it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 532/532 [00:31<00:00, 16.95it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = process_df(df_train, DATA_PATH_HEARTHSTONE_ALLSET, clip_model, clip_preprocess, False)\n",
    "train_labels = label_encoder.fit_transform(df_train['set'])\n",
    "test_data = process_df(df_test, DATA_PATH_HEARTHSTONE_ALLSET, clip_model, clip_preprocess, False)\n",
    "test_labels = label_encoder.fit_transform(df_test['set'])\n",
    "dev_data = process_df(df_dev, DATA_PATH_HEARTHSTONE_ALLSET, clip_model, clip_preprocess, False)\n",
    "dev_labels = label_encoder.fit_transform(df_dev['set'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7c32b7c1-9525-4f97-ab87-c5bc9182d57e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ALTERAC_VALLEY': 0,\n",
       " 'BATTLEGROUNDS': 1,\n",
       " 'BLACK_TEMPLE': 2,\n",
       " 'BOOMSDAY': 3,\n",
       " 'BRM': 4,\n",
       " 'CORE': 5,\n",
       " 'CREDITS': 6,\n",
       " 'DALARAN': 7,\n",
       " 'DARKMOON_FAIRE': 8,\n",
       " 'DRAGONS': 9,\n",
       " 'EXPERT1': 10,\n",
       " 'GANGS': 11,\n",
       " 'GILNEAS': 12,\n",
       " 'GVG': 13,\n",
       " 'ICECROWN': 14,\n",
       " 'KARA': 15,\n",
       " 'LEGACY': 16,\n",
       " 'LETTUCE': 17,\n",
       " 'LOE': 18,\n",
       " 'LOOTAPALOOZA': 19,\n",
       " 'MISSIONS': 20,\n",
       " 'NAXX': 21,\n",
       " 'OG': 22,\n",
       " 'PLACEHOLDER_202204': 23,\n",
       " 'REVENDRETH': 24,\n",
       " 'SCHOLOMANCE': 25,\n",
       " 'STORMWIND': 26,\n",
       " 'TAVERNS_OF_TIME': 27,\n",
       " 'TB': 28,\n",
       " 'TGT': 29,\n",
       " 'THE_BARRENS': 30,\n",
       " 'THE_SUNKEN_CITY': 31,\n",
       " 'TROLL': 32,\n",
       " 'ULDUM': 33,\n",
       " 'UNGORO': 34,\n",
       " 'VANILLA': 35,\n",
       " 'YEAR_OF_THE_DRAGON': 36}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "class_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8a8555d7-d58d-4977-a5b1-35dd9293d187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialized model\n",
    "input_size = train_data[0].size(1)\n",
    "hidden_size = 128\n",
    "num_epochs = 30\n",
    "num_classes = len(label_encoder.classes_)\n",
    "lr = 0.0002\n",
    "batch_size = 16\n",
    "    \n",
    "model = Model(input_size, hidden_size, num_classes).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "af0505d3-bd70-4459-a973-96f1d2619b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize data loader\n",
    "train_dataset = CustomDataset(train_data, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = CustomDataset(test_data, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "dev_dataset = CustomDataset(dev_data, dev_labels)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4046c9b2-9aa8-456b-84f8-c2d4436e56b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30]\n",
      "  Train Loss: 3.3281 - Validation Loss: 3.1008\n",
      "Epoch [2/30]\n",
      "  Train Loss: 2.9976 - Validation Loss: 2.8272\n",
      "Epoch [3/30]\n",
      "  Train Loss: 2.7651 - Validation Loss: 2.6517\n",
      "Epoch [4/30]\n",
      "  Train Loss: 2.6194 - Validation Loss: 2.5379\n",
      "Epoch [5/30]\n",
      "  Train Loss: 2.5090 - Validation Loss: 2.4425\n",
      "Epoch [6/30]\n",
      "  Train Loss: 2.3993 - Validation Loss: 2.3644\n",
      "Epoch [7/30]\n",
      "  Train Loss: 2.3141 - Validation Loss: 2.2883\n",
      "Epoch [8/30]\n",
      "  Train Loss: 2.2363 - Validation Loss: 2.2242\n",
      "Epoch [9/30]\n",
      "  Train Loss: 2.1665 - Validation Loss: 2.1666\n",
      "Epoch [10/30]\n",
      "  Train Loss: 2.1040 - Validation Loss: 2.1290\n",
      "Epoch [11/30]\n",
      "  Train Loss: 2.0514 - Validation Loss: 2.0840\n",
      "Epoch [12/30]\n",
      "  Train Loss: 1.9976 - Validation Loss: 2.0509\n",
      "Epoch [13/30]\n",
      "  Train Loss: 1.9567 - Validation Loss: 2.0208\n",
      "Epoch [14/30]\n",
      "  Train Loss: 1.9105 - Validation Loss: 1.9850\n",
      "Epoch [15/30]\n",
      "  Train Loss: 1.8660 - Validation Loss: 1.9631\n",
      "Epoch [16/30]\n",
      "  Train Loss: 1.8120 - Validation Loss: 1.9527\n",
      "Epoch [17/30]\n",
      "  Train Loss: 1.7753 - Validation Loss: 1.9262\n",
      "Epoch [18/30]\n",
      "  Train Loss: 1.7383 - Validation Loss: 1.9113\n",
      "Epoch [19/30]\n",
      "  Train Loss: 1.6951 - Validation Loss: 1.8885\n",
      "Epoch [20/30]\n",
      "  Train Loss: 1.6666 - Validation Loss: 1.8822\n",
      "Epoch [21/30]\n",
      "  Train Loss: 1.6410 - Validation Loss: 1.8744\n",
      "Epoch [22/30]\n",
      "  Train Loss: 1.5973 - Validation Loss: 1.8640\n",
      "Epoch [23/30]\n",
      "  Train Loss: 1.5629 - Validation Loss: 1.8482\n",
      "Epoch [24/30]\n",
      "  Train Loss: 1.5469 - Validation Loss: 1.8390\n",
      "Epoch [25/30]\n",
      "  Train Loss: 1.5050 - Validation Loss: 1.8399\n",
      "Epoch [26/30]\n",
      "  Train Loss: 1.4715 - Validation Loss: 1.8316\n",
      "Epoch [27/30]\n",
      "  Train Loss: 1.4552 - Validation Loss: 1.8284\n",
      "Epoch [28/30]\n",
      "  Train Loss: 1.4213 - Validation Loss: 1.8226\n",
      "Epoch [29/30]\n",
      "  Train Loss: 1.3876 - Validation Loss: 1.8154\n",
      "Epoch [30/30]\n",
      "  Train Loss: 1.3713 - Validation Loss: 1.8218\n",
      "Accuracy: 0.501\n",
      "Log Loss: 1.789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felix\\anaconda3\\envs\\masterthesis\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2969: UserWarning: The y_pred values do not sum to one. Starting from 1.5 thiswill result in an error.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# train model and evaluation\n",
    "train(model, train_loader, dev_loader)\n",
    "evaluate_accuracy(model, test_loader)\n",
    "evaluate_log_loss(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3497fa7-ff2d-46a0-9981-558dd61b42ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a9e1c83-517c-41db-86ea-bcad52f5b145",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "736f6a31-c6e2-4a74-80dd-aaa25dd45d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "95d70a15-1993-472b-9c8c-3d7225a1def0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = TabularDataset(f'{DATA_PATH_HEARTHSTONE_RACE}/train.csv')\n",
    "label = 'race'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9e1b1416-a8a4-4790-bbe3-27c89b939b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20240429_150521\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20240429_150521\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.11.9\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "CPU Count:          12\n",
      "Memory Avail:       22.86 GB / 31.95 GB (71.5%)\n",
      "Disk Space Avail:   226.86 GB / 465.13 GB (48.8%)\n",
      "===================================================\n",
      "Train Data Rows:    5398\n",
      "Train Data Columns: 13\n",
      "Label Column:       race\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == object).\n",
      "\tFirst 10 (of 15) unique label values:  ['NONE_race', 'BEAST', 'DRAGON', 'DEMON', 'PIRATE', 'TOTEM', 'MURLOC', 'ELEMENTAL', 'MECHANICAL', 'QUILBOAR']\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 15\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    23409.58 MB\n",
      "\tTrain Data (Original)  Memory Usage: 3.50 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['text']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 336\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 2): ['id', 'Image Path']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('object', []) : 2 | ['id', 'Image Path']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])        : 1 | ['collectible']\n",
      "\t\t('int', [])          : 3 | ['health', 'attack', 'cost']\n",
      "\t\t('object', [])       : 6 | ['cardClass', 'name', 'set', 'rarity', 'artist', ...]\n",
      "\t\t('object', ['text']) : 1 | ['text']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :   6 | ['cardClass', 'name', 'set', 'rarity', 'artist', ...]\n",
      "\t\t('category', ['text_as_category'])  :   1 | ['text']\n",
      "\t\t('int', [])                         :   3 | ['health', 'attack', 'cost']\n",
      "\t\t('int', ['binned', 'text_special']) :  18 | ['text.char_count', 'text.word_count', 'text.capital_ratio', 'text.lower_ratio', 'text.digit_ratio', ...]\n",
      "\t\t('int', ['bool'])                   :   1 | ['collectible']\n",
      "\t\t('int', ['text_ngram'])             : 317 | ['__nlp__.10', '__nlp__.add', '__nlp__.add random', '__nlp__.adjacent', '__nlp__.adjacent minions', ...]\n",
      "\t5.4s = Fit runtime\n",
      "\t11 features in original data used to generate 346 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 3.55 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 5.5s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 4858, Val Rows: 540\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "C:\\Users\\Felix\\anaconda3\\envs\\masterthesis\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] Das System kann die angegebene Datei nicht finden\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\masterthesis\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\masterthesis\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\masterthesis\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\masterthesis\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\t0.5315\t = Validation score   (accuracy)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.6259\t = Validation score   (accuracy)\n",
      "\t0.06s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t0.7852\t = Validation score   (accuracy)\n",
      "\t6.06s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\t0.7648\t = Validation score   (accuracy)\n",
      "\t9.46s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t0.7648\t = Validation score   (accuracy)\n",
      "\t11.53s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.7315\t = Validation score   (accuracy)\n",
      "\t1.24s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.7222\t = Validation score   (accuracy)\n",
      "\t1.17s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t0.7889\t = Validation score   (accuracy)\n",
      "\t363.98s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.7296\t = Validation score   (accuracy)\n",
      "\t1.11s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.7222\t = Validation score   (accuracy)\n",
      "\t1.14s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.7574\t = Validation score   (accuracy)\n",
      "\t11.84s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.6593\t = Validation score   (accuracy)\n",
      "\t20.83s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t0.7426\t = Validation score   (accuracy)\n",
      "\t18.86s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'CatBoost': 0.364, 'NeuralNetFastAI': 0.273, 'LightGBMXT': 0.091, 'LightGBM': 0.091, 'RandomForestGini': 0.091, 'LightGBMLarge': 0.091}\n",
      "\t0.8259\t = Validation score   (accuracy)\n",
      "\t0.16s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 455.93s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20240429_150521\")\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(label=label).fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b597ac54-4352-4113-96b6-c756ebb6e008",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predictor.predict(df_test.drop(columns=[label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cf8326cf-e388-49b5-ac5b-ae85e916fac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7766798418972332,\n",
       " 'balanced_accuracy': 0.6358084823002375,\n",
       " 'mcc': 0.6556862418092518}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.evaluate(df_test, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8aadb0e4-a198-4ac3-8a7a-7b504f7d6098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_test</th>\n",
       "      <th>score_val</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>pred_time_test</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_test_marginal</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>0.776680</td>\n",
       "      <td>0.825926</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>1.009249</td>\n",
       "      <td>0.264622</td>\n",
       "      <td>411.284641</td>\n",
       "      <td>0.009997</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>0.156773</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LightGBMXT</td>\n",
       "      <td>0.764822</td>\n",
       "      <td>0.764815</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.166000</td>\n",
       "      <td>0.032028</td>\n",
       "      <td>9.456308</td>\n",
       "      <td>0.166000</td>\n",
       "      <td>0.032028</td>\n",
       "      <td>9.456308</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.755929</td>\n",
       "      <td>0.757407</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.365675</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>11.841154</td>\n",
       "      <td>0.365675</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>11.841154</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.753953</td>\n",
       "      <td>0.764815</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.269999</td>\n",
       "      <td>0.069487</td>\n",
       "      <td>11.533742</td>\n",
       "      <td>0.269999</td>\n",
       "      <td>0.069487</td>\n",
       "      <td>11.533742</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ExtraTreesGini</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.729630</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.292216</td>\n",
       "      <td>0.064921</td>\n",
       "      <td>1.113638</td>\n",
       "      <td>0.292216</td>\n",
       "      <td>0.064921</td>\n",
       "      <td>1.113638</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LightGBMLarge</td>\n",
       "      <td>0.735178</td>\n",
       "      <td>0.742593</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.202001</td>\n",
       "      <td>0.036648</td>\n",
       "      <td>18.857769</td>\n",
       "      <td>0.202001</td>\n",
       "      <td>0.036648</td>\n",
       "      <td>18.857769</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ExtraTreesEntr</td>\n",
       "      <td>0.733202</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.260048</td>\n",
       "      <td>0.077563</td>\n",
       "      <td>1.143402</td>\n",
       "      <td>0.260048</td>\n",
       "      <td>0.077563</td>\n",
       "      <td>1.143402</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RandomForestGini</td>\n",
       "      <td>0.728261</td>\n",
       "      <td>0.731481</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.256249</td>\n",
       "      <td>0.065462</td>\n",
       "      <td>1.244874</td>\n",
       "      <td>0.256249</td>\n",
       "      <td>0.065462</td>\n",
       "      <td>1.244874</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.723320</td>\n",
       "      <td>0.788889</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.069002</td>\n",
       "      <td>0.038999</td>\n",
       "      <td>363.975809</td>\n",
       "      <td>0.069002</td>\n",
       "      <td>0.038999</td>\n",
       "      <td>363.975809</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NeuralNetFastAI</td>\n",
       "      <td>0.721344</td>\n",
       "      <td>0.785185</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.036001</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>6.059366</td>\n",
       "      <td>0.036001</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>6.059366</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RandomForestEntr</td>\n",
       "      <td>0.720356</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.255390</td>\n",
       "      <td>0.065529</td>\n",
       "      <td>1.167749</td>\n",
       "      <td>0.255390</td>\n",
       "      <td>0.065529</td>\n",
       "      <td>1.167749</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NeuralNetTorch</td>\n",
       "      <td>0.665020</td>\n",
       "      <td>0.659259</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.046999</td>\n",
       "      <td>0.019998</td>\n",
       "      <td>20.828613</td>\n",
       "      <td>0.046999</td>\n",
       "      <td>0.019998</td>\n",
       "      <td>20.828613</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>KNeighborsDist</td>\n",
       "      <td>0.615613</td>\n",
       "      <td>0.625926</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.050998</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>0.064999</td>\n",
       "      <td>0.050998</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>0.064999</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>KNeighborsUnif</td>\n",
       "      <td>0.540514</td>\n",
       "      <td>0.531481</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.047161</td>\n",
       "      <td>0.120218</td>\n",
       "      <td>0.048676</td>\n",
       "      <td>0.047161</td>\n",
       "      <td>0.120218</td>\n",
       "      <td>0.048676</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  model  score_test  score_val eval_metric  pred_time_test  \\\n",
       "0   WeightedEnsemble_L2    0.776680   0.825926    accuracy        1.009249   \n",
       "1            LightGBMXT    0.764822   0.764815    accuracy        0.166000   \n",
       "2               XGBoost    0.755929   0.757407    accuracy        0.365675   \n",
       "3              LightGBM    0.753953   0.764815    accuracy        0.269999   \n",
       "4        ExtraTreesGini    0.739130   0.729630    accuracy        0.292216   \n",
       "5         LightGBMLarge    0.735178   0.742593    accuracy        0.202001   \n",
       "6        ExtraTreesEntr    0.733202   0.722222    accuracy        0.260048   \n",
       "7      RandomForestGini    0.728261   0.731481    accuracy        0.256249   \n",
       "8              CatBoost    0.723320   0.788889    accuracy        0.069002   \n",
       "9       NeuralNetFastAI    0.721344   0.785185    accuracy        0.036001   \n",
       "10     RandomForestEntr    0.720356   0.722222    accuracy        0.255390   \n",
       "11       NeuralNetTorch    0.665020   0.659259    accuracy        0.046999   \n",
       "12       KNeighborsDist    0.615613   0.625926    accuracy        0.050998   \n",
       "13       KNeighborsUnif    0.540514   0.531481    accuracy        0.047161   \n",
       "\n",
       "    pred_time_val    fit_time  pred_time_test_marginal  \\\n",
       "0        0.264622  411.284641                 0.009997   \n",
       "1        0.032028    9.456308                 0.166000   \n",
       "2        0.036000   11.841154                 0.365675   \n",
       "3        0.069487   11.533742                 0.269999   \n",
       "4        0.064921    1.113638                 0.292216   \n",
       "5        0.036648   18.857769                 0.202001   \n",
       "6        0.077563    1.143402                 0.260048   \n",
       "7        0.065462    1.244874                 0.256249   \n",
       "8        0.038999  363.975809                 0.069002   \n",
       "9        0.021000    6.059366                 0.036001   \n",
       "10       0.065529    1.167749                 0.255390   \n",
       "11       0.019998   20.828613                 0.046999   \n",
       "12       0.022000    0.064999                 0.050998   \n",
       "13       0.120218    0.048676                 0.047161   \n",
       "\n",
       "    pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       "0                 0.000998           0.156773            2       True   \n",
       "1                 0.032028           9.456308            1       True   \n",
       "2                 0.036000          11.841154            1       True   \n",
       "3                 0.069487          11.533742            1       True   \n",
       "4                 0.064921           1.113638            1       True   \n",
       "5                 0.036648          18.857769            1       True   \n",
       "6                 0.077563           1.143402            1       True   \n",
       "7                 0.065462           1.244874            1       True   \n",
       "8                 0.038999         363.975809            1       True   \n",
       "9                 0.021000           6.059366            1       True   \n",
       "10                0.065529           1.167749            1       True   \n",
       "11                0.019998          20.828613            1       True   \n",
       "12                0.022000           0.064999            1       True   \n",
       "13                0.120218           0.048676            1       True   \n",
       "\n",
       "    fit_order  \n",
       "0          14  \n",
       "1           4  \n",
       "2          11  \n",
       "3           5  \n",
       "4           9  \n",
       "5          13  \n",
       "6          10  \n",
       "7           6  \n",
       "8           8  \n",
       "9           3  \n",
       "10          7  \n",
       "11         12  \n",
       "12          2  \n",
       "13          1  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.leaderboard(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de936555-5611-4b52-a97c-0a84a6d3852d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterthesis",
   "language": "python",
   "name": "masterthesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
