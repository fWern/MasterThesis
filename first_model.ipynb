{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7ff9891-f8b6-42e9-8183-ae390474399a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felix\\anaconda3\\envs\\masterthesis\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import log_loss\n",
    "import clip\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from autogluon.features.generators import AutoMLPipelineFeatureGenerator\n",
    "from autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch import TabularNeuralNetTorchModel\n",
    "from autogluon.core.constants import MULTICLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a20b6ca-25bb-475e-acb6-c042afa71add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9bf3333-4c1c-4492-938e-173a8e8fe45e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib._GeneratorContextManager at 0x1cddfc39510>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.cuda.sdp_kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "013a36ef-4a61-4cff-b45c-d4c6b391a128",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.enable_flash_sdp(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6f84ad8-0477-45b4-be9c-977b4a5ecdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths \n",
    "\n",
    "# hearthstone\n",
    "DATA_PATH_HEARTHSTONE_RACE = '../dataset/Hearthstone-Minion-race/' #label: race\n",
    "DATA_PATH_HEARTHSTONE_CARDCLASS = '../dataset/Hearthstone-All-cardClass/' # label: card class\n",
    "DATA_PATH_HEARTHSTONE_ALLSET = '../dataset/Hearthstone-All-set/' # label: set\n",
    "DATA_PATH_HEARTHSTONE_SPELLSCHOOL = '../dataset/Hearthstone-Spell-spellSchool/' # label: set\n",
    "\n",
    "# pokemon\n",
    "DATA_PATH_POKEMON_SECONDARY = '../dataset/Pokemon-secondary_type/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89693f58-abfb-4c84-9a01-51affab8f8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8992900-38ec-496d-9d01-5b42cc656729",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e92854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df, id):\n",
    "    df['text'] = df['text'].fillna('unknown')\n",
    "    df['artist'] = df['artist'].fillna('unknown')\n",
    "    df['mechanics'] = df['mechanics'].fillna('unknown')\n",
    "    if id:\n",
    "        df['combined_text'] = df['name'].str.lower() + ' ' + df['id'].str.lower()  + ' ' + df['artist'].str.lower()  + ' ' + df['text'].str.lower()  + ' ' + df['mechanics'].str.lower() \n",
    "    else:\n",
    "        df['combined_text'] = df['name'].str.lower() + ' ' + df['artist'].str.lower()  + ' ' + df['text'].str.lower()  + ' ' + df['mechanics'].str.lower() \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35e6992a-6cc8-48c6-9d20-ebcb7cad5bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_image_text(texts, images, tab_features, clip_model, clip_preprocess):\n",
    "    image_text = []\n",
    "    label_list = []\n",
    "    tab_features = tab_features.to(device)\n",
    "    for idx in tqdm(range(len(texts))):\n",
    "        text = texts[idx]\n",
    "        image = images[idx]\n",
    "        text = clip.tokenize(text, truncate=True).to(device)\n",
    "        image = clip_preprocess(Image.open(image)).unsqueeze(0).to(device)\n",
    "        tab_feature = tab_features[idx].unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            text_features = clip_model.encode_text(text)\n",
    "            image_features = clip_model.encode_image(image)\n",
    "        combined_features = torch.cat((text_features, image_features, tab_feature), 1)\n",
    "        image_text.append(combined_features)\n",
    "    return image_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "306b9e45-19f5-44b4-8ea6-3ed9777fa16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(df, data_path, tab_features, clip_model, clip_preprocess, id):\n",
    "    df = preprocess_df(df, id)\n",
    "    texts = list(df['combined_text'])\n",
    "    images = [data_path + img for img in list(df['Image Path'])]\n",
    "    data = combine_image_text(texts, images, tab_features, clip_model, clip_preprocess)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d644e892-6507-43fb-a983-69ecb66b78de",
   "metadata": {},
   "source": [
    "# Model Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad92820f-16f1-46dd-aa43-c7296da5f0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features[idx].clone().detach().to(torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d34832e9-eba6-44ed-8b14-a6d6d1992c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc_1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc_2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc_3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.batch_norm_1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.batch_norm_2 = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = self.fc_1(x)\n",
    "        x = self.batch_norm_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc_2(x)\n",
    "        x = self.batch_norm_2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc_3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "053cf1e8-e3d8-4f98-b1bb-be753364de6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, dev_loader):\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5, verbose=True)\n",
    "    patience = 5\n",
    "    best_val_loss = np.inf\n",
    "    early_stop_counter = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        model.train()\n",
    "        for feature, label in train_loader:\n",
    "            feature = feature.to(device)\n",
    "            label = label.to(device)\n",
    "            output = model(feature)\n",
    "            loss = criterion(output, label)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * feature.size(0)\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for dev_feature, dev_labels in dev_loader:\n",
    "                dev_feature = dev_feature.to(device)\n",
    "                dev_labels = dev_labels.to(device)\n",
    "                output = model(dev_feature)\n",
    "                loss = criterion(output, dev_labels)\n",
    "                val_loss += loss.item() * dev_feature.size(0)\n",
    "                    \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        val_loss /= len(dev_loader.dataset)\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f} - Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        if early_stop_counter >= patience:\n",
    "            print(\"Early stopping.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01998613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, test_loader):\n",
    "    total_accuracy = 0.0\n",
    "    total_samples = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for feature, label in test_loader:\n",
    "            feature = feature.to(device)\n",
    "            label = label.to(device)\n",
    "            output = model(feature)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            correct = (predicted == label).sum().item()\n",
    "            total_accuracy += correct\n",
    "            total_samples += label.size(0)\n",
    "    accuracy = total_accuracy / total_samples\n",
    "    print(f\"Accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92972632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_log_loss(model, test_loader, labels):\n",
    "    model.eval()\n",
    "    log = 0.0\n",
    "    with torch.no_grad():\n",
    "        for feature, label in test_loader:\n",
    "            feature = feature.to(device)\n",
    "            label = label.to(device)\n",
    "            output = model(feature)\n",
    "            probabilities = F.softmax(output, dim=1)\n",
    "            log += log_loss(label.cpu().numpy(), probabilities.cpu().numpy(), labels=labels)\n",
    "    log = log / len(test_loader)\n",
    "    print(f\"Log Loss: {log:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cc0a312-225e-4f83-85d7-86c9549cb0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tab_features(df, labels):\n",
    "    auto_ml = AutoMLPipelineFeatureGenerator(\n",
    "    enable_text_special_features=False, \n",
    "    enable_text_ngram_features=False, \n",
    "    enable_vision_features=False,\n",
    "    )\n",
    "    auto_ml.fit(df)\n",
    "    #auto_ml.print_feature_metadata_info(log_level=40)\n",
    "\n",
    "    features = auto_ml.transform(df)\n",
    "\n",
    "    tab_model = TabularNeuralNetTorchModel()\n",
    "    tab_model.problem_type = MULTICLASS\n",
    "    tab_model.quantile_levels = None\n",
    "    tab_model._set_default_params()\n",
    "    params = tab_model._get_model_params()\n",
    "    processor_kwargs, optimizer_kwargs, fit_kwargs, loss_kwargs, params = tab_model._prepare_params(params)\n",
    "    tab_model._preprocess_set_features(features)\n",
    "    dataset, _ = tab_model._generate_datasets(features, labels, processor_kwargs)\n",
    "    return torch.tensor(tab_model.processor.transform(features), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9176e02-7505-4c79-9960-732af23c7219",
   "metadata": {},
   "source": [
    "# Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c6416f5-444e-45a2-b52a-21b801722d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(DATA_PATH_HEARTHSTONE_ALLSET + \"/train.csv\")\n",
    "df_test = pd.read_csv(DATA_PATH_HEARTHSTONE_ALLSET + \"/test.csv\")\n",
    "df_dev = pd.read_csv(DATA_PATH_HEARTHSTONE_ALLSET + \"/dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9102e162-ed3d-42f3-b41c-2e6bd2b9770e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_data_train = df_train[['cardClass', 'health', 'attack', 'cost', 'rarity', 'type', 'collectible', 'spellSchool', 'race', 'durability', 'overload', 'spellDamage']].copy()\n",
    "tab_data_test = df_train[['cardClass', 'health', 'attack', 'cost', 'rarity', 'type', 'collectible', 'spellSchool', 'race', 'durability', 'overload', 'spellDamage']].copy()\n",
    "tab_data_dev = df_train[['cardClass', 'health', 'attack', 'cost', 'rarity', 'type', 'collectible', 'spellSchool', 'race', 'durability', 'overload', 'spellDamage']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf3cfe0c-9ded-4416-82cd-f8988f4db2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "train_labels = le.fit_transform(df_train['set'])\n",
    "test_labels = le.transform(df_test['set'])\n",
    "dev_labels = le.transform(df_dev['set'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c38c82e-e16d-4e4a-9681-6c00cb39cd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_features_train = get_tab_features(tab_data_train, train_labels)\n",
    "tab_features_test = get_tab_features(tab_data_test, train_labels)\n",
    "tab_features_dev = get_tab_features(tab_data_dev, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a54035ba-128a-4793-bb41-49d07d55cc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 8548/8548 [12:41<00:00, 11.22it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1603/1603 [02:18<00:00, 11.58it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 532/532 [00:53<00:00, 10.02it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = process_df(df_train, DATA_PATH_HEARTHSTONE_ALLSET, tab_features_train, clip_model, clip_preprocess, False)\n",
    "test_data = process_df(df_test, DATA_PATH_HEARTHSTONE_ALLSET, tab_features_test, clip_model, clip_preprocess, False)\n",
    "dev_data = process_df(df_dev, DATA_PATH_HEARTHSTONE_ALLSET, tab_features_dev, clip_model, clip_preprocess, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a8555d7-d58d-4977-a5b1-35dd9293d187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialized model\n",
    "input_size = train_data[0].size(1)\n",
    "hidden_size = 128\n",
    "num_epochs = 30\n",
    "num_classes = len(le.classes_)\n",
    "lr = 0.0002\n",
    "batch_size = 8\n",
    "    \n",
    "model = Model(input_size, hidden_size, num_classes).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af0505d3-bd70-4459-a973-96f1d2619b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize data loader\n",
    "train_dataset = CustomDataset(train_data, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = CustomDataset(test_data, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "dev_dataset = CustomDataset(dev_data, dev_labels)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4046c9b2-9aa8-456b-84f8-c2d4436e56b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30]\n",
      "  Train Loss: 3.1638 - Validation Loss: 3.1629\n",
      "Epoch [2/30]\n",
      "  Train Loss: 2.7841 - Validation Loss: 2.9769\n",
      "Epoch [3/30]\n",
      "  Train Loss: 2.5876 - Validation Loss: 2.8535\n",
      "Epoch [4/30]\n",
      "  Train Loss: 2.4450 - Validation Loss: 2.6927\n",
      "Epoch [5/30]\n",
      "  Train Loss: 2.3098 - Validation Loss: 2.6129\n",
      "Epoch [6/30]\n",
      "  Train Loss: 2.2220 - Validation Loss: 2.5681\n",
      "Epoch [7/30]\n",
      "  Train Loss: 2.1579 - Validation Loss: 2.4915\n",
      "Epoch [8/30]\n",
      "  Train Loss: 2.0882 - Validation Loss: 2.4267\n",
      "Epoch [9/30]\n",
      "  Train Loss: 2.0371 - Validation Loss: 2.4278\n",
      "Epoch [10/30]\n",
      "  Train Loss: 2.0003 - Validation Loss: 2.4236\n",
      "Epoch [11/30]\n",
      "  Train Loss: 1.9480 - Validation Loss: 2.3686\n",
      "Epoch [12/30]\n",
      "  Train Loss: 1.9039 - Validation Loss: 2.4396\n",
      "Epoch [13/30]\n",
      "  Train Loss: 1.8616 - Validation Loss: 2.3264\n",
      "Epoch [14/30]\n",
      "  Train Loss: 1.8323 - Validation Loss: 2.3489\n",
      "Epoch [15/30]\n",
      "  Train Loss: 1.7889 - Validation Loss: 2.3025\n",
      "Epoch [16/30]\n",
      "  Train Loss: 1.7726 - Validation Loss: 2.2800\n",
      "Epoch [17/30]\n",
      "  Train Loss: 1.7854 - Validation Loss: 2.2462\n",
      "Epoch [18/30]\n",
      "  Train Loss: 1.7573 - Validation Loss: 2.2689\n",
      "Epoch [19/30]\n",
      "  Train Loss: 1.7208 - Validation Loss: 2.2678\n",
      "Epoch [20/30]\n",
      "  Train Loss: 1.6775 - Validation Loss: 2.3019\n",
      "Epoch [21/30]\n",
      "  Train Loss: 1.6575 - Validation Loss: 2.3192\n",
      "Epoch [22/30]\n",
      "  Train Loss: 1.6615 - Validation Loss: 2.3211\n",
      "Early stopping.\n",
      "Accuracy: 0.349\n",
      "Log Loss: 2.475\n"
     ]
    }
   ],
   "source": [
    "# train model and evaluation\n",
    "train(model, train_loader, dev_loader)\n",
    "evaluate_accuracy(model, test_loader)\n",
    "evaluate_log_loss(model, test_loader, np.unique(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c62cc2-ca0e-417c-87ad-ad369f69ffaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "ab2443a6-a639-4f72-907f-af5208999452",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, encoding_dim)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "aeb0a4b9-a60e-42ff-87a9-4db94a9b8201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "td = scaler.fit_transform(tab_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "e30650ce-04b7-4f54-9d7d-3d3ac4379766",
   "metadata": {},
   "outputs": [],
   "source": [
    "td = torch.tensor(td, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "60c51eae-221f-4501-b6de-4ee95316022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = td.shape[1]\n",
    "encoding_dim = 32\n",
    "num_epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "model = AutoEncoder(input_dim, encoding_dim).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "190d4a85-3db6-45eb-b11d-cceb070da8e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16x12 and 1056x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[267], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m inputs \u001b[38;5;241m=\u001b[39m td[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      4\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 5\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m      6\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, inputs)\n\u001b[0;32m      7\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\masterthesis\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\masterthesis\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 12\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \n\u001b[1;32m---> 12\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_1(x)))\n\u001b[0;32m     13\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_2(out)))\n\u001b[0;32m     14\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_3(out)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\masterthesis\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\masterthesis\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\masterthesis\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16x12 and 1056x128)"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(td), batch_size):\n",
    "        inputs = td[i:i+batch_size].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}/{num_epochs} - Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "ebe5a09c-03a1-4102-b5d3-9a1d203fad23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8379,  0.1642, -0.7523,  ..., -1.2896,  1.2782,  3.5073],\n",
       "        [-2.4339,  0.9879,  0.4982,  ..., -2.3031,  0.5396, -0.4554],\n",
       "        [ 0.0853,  1.5901, -0.7655,  ...,  3.1013, -1.0537,  1.5004],\n",
       "        ...,\n",
       "        [-0.2787,  3.8619,  0.1558,  ..., -0.0656,  0.1702,  1.7958],\n",
       "        [ 0.5740,  1.6517, -0.1291,  ..., -5.8594,  1.3804, -0.7474],\n",
       "        [-1.1256,  3.7655,  0.2812,  ...,  0.7918,  0.1051,  1.8746]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = model.encoder.to(device)\n",
    "tab_features = encoder(td.to(device))\n",
    "tab_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b244a08-8e75-4b4c-a010-27b5a13f5d15",
   "metadata": {},
   "source": [
    "# Pokemon secondary type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8222a3f1-d7eb-45c2-b384-0185fa077555",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(DATA_PATH_POKEMON_SECONDARY + \"/train.csv\")\n",
    "df_test = pd.read_csv(DATA_PATH_POKEMON_SECONDARY + \"/test.csv\")\n",
    "df_dev = pd.read_csv(DATA_PATH_POKEMON_SECONDARY + \"/dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7818a3eb-a738-4ad8-9dcf-0d9333acf569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['name', 'generation', 'status', 'species', 'type_2', 'height_m',\n",
       "       'weight_kg', 'abilities_number', 'ability_1', 'ability_2',\n",
       "       'ability_hidden', 'total_points', 'hp', 'attack', 'defense',\n",
       "       'sp_attack', 'sp_defense', 'speed', 'catch_rate', 'base_friendship',\n",
       "       'base_experience', 'growth_rate', 'percentage_male', 'Image Path'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c42a8166-3636-4796-b382-8ad31ab78119",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_data_train = df_train[['generation', 'status', 'height_m', 'weight_kg', 'abilities_number', 'total_points', 'hp', 'attack', 'defense', 'sp_attack', 'sp_defense', 'speed', 'catch_rate', 'base_friendship', 'base_experience', 'growth_rate', 'percentage_male']].copy()\n",
    "tab_data_test = df_train[['generation', 'status', 'height_m', 'weight_kg', 'abilities_number', 'total_points', 'hp', 'attack', 'defense', 'sp_attack', 'sp_defense', 'speed', 'catch_rate', 'base_friendship', 'base_experience', 'growth_rate', 'percentage_male']].copy()\n",
    "tab_data_dev = df_train[['generation', 'status', 'height_m', 'weight_kg', 'abilities_number', 'total_points', 'hp', 'attack', 'defense', 'sp_attack', 'sp_defense', 'speed', 'catch_rate', 'base_friendship', 'base_experience', 'growth_rate', 'percentage_male']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b712142b-30a8-4c83-9fbd-da0d29c853f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "train_labels = le.fit_transform(df_train['type_2'])\n",
    "test_labels = le.transform(df_test['type_2'])\n",
    "dev_labels = le.transform(df_dev['type_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd3209cc-10d6-4cf9-bfe6-a85328599241",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_features_train = get_tab_features(tab_data_train, train_labels)\n",
    "tab_features_test = get_tab_features(tab_data_test, train_labels)\n",
    "tab_features_dev = get_tab_features(tab_data_dev, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "94aa07a3-fef4-4deb-b818-463ff4e8b160",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['combined'] = df_train['name'] + ' ' + df_train['species']  + ' ' + df_train['ability_1'] + ' ' + df_train['ability_2'] + ' ' + df_train['ability_hidden']\n",
    "df_test['combined'] = df_train['name'] + ' ' + df_train['species']  + ' ' + df_train['ability_1'] + ' ' + df_train['ability_2'] + ' ' + df_train['ability_hidden']\n",
    "df_dev['combined'] = df_train['name'] + ' ' + df_train['species']  + ' ' + df_train['ability_1'] + ' ' + df_train['ability_2'] + ' ' + df_train['ability_hidden']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93a4a29e-5579-42a7-aa32-7ece202df033",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train = list(df_train['combined'])\n",
    "images_train = [DATA_PATH_POKEMON_SECONDARY + img for img in list(df_train['Image Path'])]\n",
    "texts_test = list(df_test['combined'])\n",
    "images_test = [DATA_PATH_POKEMON_SECONDARY + img for img in list(df_test['Image Path'])]\n",
    "texts_dev = list(df_dev['combined'])\n",
    "images_dev = [DATA_PATH_POKEMON_SECONDARY + img for img in list(df_test['Image Path'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1591f6f-0d00-4bac-a0a3-73c8348c0168",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 719/719 [01:07<00:00, 10.63it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 133/133 [00:11<00:00, 11.17it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 45/45 [00:03<00:00, 11.97it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = combine_image_text(texts_train, images_train, tab_features_train, clip_model, clip_preprocess)\n",
    "test_data = combine_image_text(texts_test, images_test, tab_features_test, clip_model, clip_preprocess)\n",
    "dev_data = combine_image_text(texts_dev, images_dev, tab_features_dev, clip_model, clip_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c7c7ce2-b6ff-4106-ac52-998a83673ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialized model\n",
    "input_size = train_data[0].size(1)\n",
    "hidden_size = 128\n",
    "num_epochs = 30\n",
    "num_classes = len(le.classes_)\n",
    "lr = 0.00003\n",
    "batch_size = 8\n",
    "    \n",
    "model = Model(input_size, hidden_size, num_classes).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2a9d41d1-2f59-4fc3-af64-320f9729b0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize data loader\n",
    "train_dataset = CustomDataset(train_data, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = CustomDataset(test_data, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "dev_dataset = CustomDataset(dev_data, dev_labels)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eb9adcd9-8802-46e5-a193-b49b48b433a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30]\n",
      "  Train Loss: 3.1404 - Validation Loss: 3.0287\n",
      "Epoch [2/30]\n",
      "  Train Loss: 2.9921 - Validation Loss: 2.9797\n",
      "Epoch [3/30]\n",
      "  Train Loss: 2.9318 - Validation Loss: 2.9519\n",
      "Epoch [4/30]\n",
      "  Train Loss: 2.8017 - Validation Loss: 2.8913\n",
      "Epoch [5/30]\n",
      "  Train Loss: 2.7166 - Validation Loss: 2.8460\n",
      "Epoch [6/30]\n",
      "  Train Loss: 2.6257 - Validation Loss: 2.8328\n",
      "Epoch [7/30]\n",
      "  Train Loss: 2.5312 - Validation Loss: 2.7861\n",
      "Epoch [8/30]\n",
      "  Train Loss: 2.4804 - Validation Loss: 2.7527\n",
      "Epoch [9/30]\n",
      "  Train Loss: 2.3750 - Validation Loss: 2.7347\n",
      "Epoch [10/30]\n",
      "  Train Loss: 2.3077 - Validation Loss: 2.6979\n",
      "Epoch [11/30]\n",
      "  Train Loss: 2.2343 - Validation Loss: 2.7142\n",
      "Epoch [12/30]\n",
      "  Train Loss: 2.1562 - Validation Loss: 2.6674\n",
      "Epoch [13/30]\n",
      "  Train Loss: 2.0867 - Validation Loss: 2.6609\n",
      "Epoch [14/30]\n",
      "  Train Loss: 2.0472 - Validation Loss: 2.6344\n",
      "Epoch [15/30]\n",
      "  Train Loss: 1.9929 - Validation Loss: 2.6308\n",
      "Epoch [16/30]\n",
      "  Train Loss: 1.9688 - Validation Loss: 2.6274\n",
      "Epoch [17/30]\n",
      "  Train Loss: 1.8826 - Validation Loss: 2.6101\n",
      "Epoch [18/30]\n",
      "  Train Loss: 1.8711 - Validation Loss: 2.5845\n",
      "Epoch [19/30]\n",
      "  Train Loss: 1.8460 - Validation Loss: 2.5548\n",
      "Epoch [20/30]\n",
      "  Train Loss: 1.8162 - Validation Loss: 2.5723\n",
      "Epoch [21/30]\n",
      "  Train Loss: 1.7720 - Validation Loss: 2.5637\n",
      "Epoch [22/30]\n",
      "  Train Loss: 1.7224 - Validation Loss: 2.5784\n",
      "Epoch [23/30]\n",
      "  Train Loss: 1.7125 - Validation Loss: 2.5611\n",
      "Epoch [24/30]\n",
      "  Train Loss: 1.6707 - Validation Loss: 2.5746\n",
      "Early stopping.\n",
      "Accuracy: 0.504\n",
      "Log Loss: 2.211\n"
     ]
    }
   ],
   "source": [
    "# train model and evaluation\n",
    "train(model, train_loader, dev_loader)\n",
    "evaluate_accuracy(model, test_loader)\n",
    "evaluate_log_loss(model, test_loader, np.unique(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aced59a-21c7-4af7-b4be-8d58d94bb079",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ad2d3f61-d43a-42d9-9bb9-c44bd0e0b8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df_train = pd.read_csv(DATA_PATH_HEARTHSTONE_RACE + \"/train.csv\")\n",
    "df_test = pd.read_csv(DATA_PATH_HEARTHSTONE_RACE + \"/test.csv\")\n",
    "df_dev = pd.read_csv(DATA_PATH_HEARTHSTONE_RACE + \"/dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2c7c2d46-e98a-444b-831c-9e5b7648e612",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_data_train = df_train[['cardClass', 'health', 'attack', 'cost', 'rarity', 'set', 'collectible']].copy()\n",
    "tab_data_test = df_train[['cardClass', 'health', 'attack', 'cost', 'rarity', 'set', 'collectible']].copy()\n",
    "tab_data_dev = df_train[['cardClass', 'health', 'attack', 'cost', 'rarity', 'set', 'collectible']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3e3ee0db-0a28-4109-8088-bd936517d01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(df_train['race'])\n",
    "test_labels = label_encoder.transform(df_test['race'])\n",
    "dev_labels = label_encoder.transform(df_dev['race'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8b36ad56-099f-4c4b-b40d-4a9208d5a4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_features_train = get_tab_features(tab_data_train, train_labels)\n",
    "tab_features_test = get_tab_features(tab_data_test, train_labels)\n",
    "tab_features_dev = get_tab_features(tab_data_dev, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a708c7d3-8d0c-4855-a577-76cf2cfd9208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5398/5398 [07:18<00:00, 12.30it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1012/1012 [01:22<00:00, 12.30it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 337/337 [00:27<00:00, 12.32it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = process_df(df_train, DATA_PATH_HEARTHSTONE_RACE, tab_features_train, clip_model, clip_preprocess, True)\n",
    "test_data = process_df(df_test, DATA_PATH_HEARTHSTONE_RACE, tab_features_test, clip_model, clip_preprocess, True)\n",
    "dev_data = process_df(df_dev, DATA_PATH_HEARTHSTONE_RACE, tab_features_dev, clip_model, clip_preprocess, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8d9bdaaf-210c-408d-8bc6-ac0a9a49f3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialized model\n",
    "input_size = train_data[0].size(1)\n",
    "hidden_size = 128\n",
    "num_epochs = 20\n",
    "num_classes = len(label_encoder.classes_)\n",
    "lr = 0.0002\n",
    "batch_size = 16\n",
    "    \n",
    "model = Model(input_size, hidden_size, num_classes).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4da8a629-931a-48d1-9efb-53cf9bba2cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize data loader\n",
    "train_dataset = CustomDataset(train_data, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = CustomDataset(test_data, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "dev_dataset = CustomDataset(dev_data, dev_labels)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "266af29e-99ee-4c1a-bf50-b7a9ae2f9059",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20]\n",
      "  Train Loss: 1.7962 - Validation Loss: 1.4458\n",
      "Epoch [2/20]\n",
      "  Train Loss: 1.1733 - Validation Loss: 1.1246\n",
      "Epoch [3/20]\n",
      "  Train Loss: 0.9680 - Validation Loss: 0.9347\n",
      "Epoch [4/20]\n",
      "  Train Loss: 0.8458 - Validation Loss: 0.8620\n",
      "Epoch [5/20]\n",
      "  Train Loss: 0.7260 - Validation Loss: 0.7612\n",
      "Epoch [6/20]\n",
      "  Train Loss: 0.6425 - Validation Loss: 0.7364\n",
      "Epoch [7/20]\n",
      "  Train Loss: 0.5781 - Validation Loss: 0.6983\n",
      "Epoch [8/20]\n",
      "  Train Loss: 0.5305 - Validation Loss: 0.6417\n",
      "Epoch [9/20]\n",
      "  Train Loss: 0.4793 - Validation Loss: 0.6202\n",
      "Epoch [10/20]\n",
      "  Train Loss: 0.4579 - Validation Loss: 0.6092\n",
      "Epoch [11/20]\n",
      "  Train Loss: 0.4144 - Validation Loss: 0.5714\n",
      "Epoch [12/20]\n",
      "  Train Loss: 0.3907 - Validation Loss: 0.5621\n",
      "Epoch [13/20]\n",
      "  Train Loss: 0.3579 - Validation Loss: 0.5595\n",
      "Epoch [14/20]\n",
      "  Train Loss: 0.3387 - Validation Loss: 0.5740\n",
      "Epoch [15/20]\n",
      "  Train Loss: 0.3173 - Validation Loss: 0.5794\n",
      "Epoch [16/20]\n",
      "  Train Loss: 0.2902 - Validation Loss: 0.5752\n",
      "Epoch [17/20]\n",
      "  Train Loss: 0.2792 - Validation Loss: 0.5793\n",
      "Epoch [18/20]\n",
      "  Train Loss: 0.2606 - Validation Loss: 0.5625\n",
      "Early stopping.\n",
      "Accuracy: 0.858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felix\\anaconda3\\envs\\masterthesis\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2969: UserWarning: The y_pred values do not sum to one. Starting from 1.5 thiswill result in an error.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Felix\\anaconda3\\envs\\masterthesis\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2969: UserWarning: The y_pred values do not sum to one. Starting from 1.5 thiswill result in an error.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Felix\\anaconda3\\envs\\masterthesis\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2969: UserWarning: The y_pred values do not sum to one. Starting from 1.5 thiswill result in an error.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4686131321889779"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model and evaluation\n",
    "train(model, train_loader, dev_loader)\n",
    "evaluate_accuracy(model, test_loader)\n",
    "evaluate_log_loss(model, test_loader, np.unique(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0106e919-3ff4-4bb8-af2c-70f8314b33ea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Card Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d854e671-0f05-4895-aebc-a9b985404d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(DATA_PATH_HEARTHSTONE_CARDCLASS + \"/train.csv\")\n",
    "df_test = pd.read_csv(DATA_PATH_HEARTHSTONE_CARDCLASS + \"/test.csv\")\n",
    "df_dev = pd.read_csv(DATA_PATH_HEARTHSTONE_CARDCLASS + \"/dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a063613c-9971-463a-9078-b807f2a20c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2daf29ab-3c14-49a3-8bfe-3f076e7cefc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 8561/8561 [08:51<00:00, 16.10it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1603/1603 [01:34<00:00, 16.96it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 536/536 [00:32<00:00, 16.51it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = process_df(df_train, DATA_PATH_HEARTHSTONE_CARDCLASS, clip_model, clip_preprocess, True)\n",
    "train_labels = label_encoder.fit_transform(df_train['cardClass'])\n",
    "test_data = process_df(df_test, DATA_PATH_HEARTHSTONE_CARDCLASS, clip_model, clip_preprocess, True)\n",
    "test_labels = label_encoder.fit_transform(df_test['cardClass'])\n",
    "dev_data = process_df(df_dev, DATA_PATH_HEARTHSTONE_CARDCLASS, clip_model, clip_preprocess, True)\n",
    "dev_labels = label_encoder.fit_transform(df_dev['cardClass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d785fd79-dffb-4a6c-b54a-c845c8592f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DEATHKNIGHT': 0,\n",
       " 'DEMONHUNTER': 1,\n",
       " 'DRUID': 2,\n",
       " 'HUNTER': 3,\n",
       " 'MAGE': 4,\n",
       " 'NEUTRAL': 5,\n",
       " 'NONE_cardClass': 6,\n",
       " 'PALADIN': 7,\n",
       " 'PRIEST': 8,\n",
       " 'ROGUE': 9,\n",
       " 'SHAMAN': 10,\n",
       " 'WARLOCK': 11,\n",
       " 'WARRIOR': 12}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "class_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "456606f0-1015-479e-819a-5bc55e5956e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialized model\n",
    "input_size = train_data[0].size(1)\n",
    "hidden_size = 128\n",
    "num_epochs = 20\n",
    "num_classes = len(label_encoder.classes_)\n",
    "lr = 0.0002\n",
    "batch_size = 16\n",
    "    \n",
    "model = Model(input_size, hidden_size, num_classes).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "facb88a4-2737-4cd6-9eed-4660d7c7e223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize data loader\n",
    "train_dataset = CustomDataset(train_data, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = CustomDataset(test_data, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "dev_dataset = CustomDataset(dev_data, dev_labels)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "75541eda-cb1c-4dd3-9360-aeb4b7fcbd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20]\n",
      "  Train Loss: 1.9485 - Validation Loss: 1.7714\n",
      "Epoch [2/20]\n",
      "  Train Loss: 1.7056 - Validation Loss: 1.5524\n",
      "Epoch [3/20]\n",
      "  Train Loss: 1.5532 - Validation Loss: 1.4228\n",
      "Epoch [4/20]\n",
      "  Train Loss: 1.4379 - Validation Loss: 1.3060\n",
      "Epoch [5/20]\n",
      "  Train Loss: 1.3421 - Validation Loss: 1.2318\n",
      "Epoch [6/20]\n",
      "  Train Loss: 1.2709 - Validation Loss: 1.1834\n",
      "Epoch [7/20]\n",
      "  Train Loss: 1.2020 - Validation Loss: 1.1418\n",
      "Epoch [8/20]\n",
      "  Train Loss: 1.1479 - Validation Loss: 1.1007\n",
      "Epoch [9/20]\n",
      "  Train Loss: 1.1073 - Validation Loss: 1.0693\n",
      "Epoch [10/20]\n",
      "  Train Loss: 1.0538 - Validation Loss: 1.0493\n",
      "Epoch [11/20]\n",
      "  Train Loss: 1.0113 - Validation Loss: 1.0362\n",
      "Epoch [12/20]\n",
      "  Train Loss: 0.9733 - Validation Loss: 1.0018\n",
      "Epoch [13/20]\n",
      "  Train Loss: 0.9300 - Validation Loss: 0.9919\n",
      "Epoch [14/20]\n",
      "  Train Loss: 0.8939 - Validation Loss: 0.9684\n",
      "Epoch [15/20]\n",
      "  Train Loss: 0.8541 - Validation Loss: 0.9799\n",
      "Epoch [16/20]\n",
      "  Train Loss: 0.8358 - Validation Loss: 0.9913\n",
      "Epoch [17/20]\n",
      "  Train Loss: 0.8070 - Validation Loss: 0.9536\n",
      "Epoch [18/20]\n",
      "  Train Loss: 0.7666 - Validation Loss: 0.9651\n",
      "Epoch [19/20]\n",
      "  Train Loss: 0.7476 - Validation Loss: 0.9061\n",
      "Epoch [20/20]\n",
      "  Train Loss: 0.7150 - Validation Loss: 0.9455\n",
      "Accuracy: 0.667\n",
      "Log Loss: 1.089\n"
     ]
    }
   ],
   "source": [
    "# train model and evaluation\n",
    "train(model, train_loader, dev_loader)\n",
    "evaluate_accuracy(model, test_loader)\n",
    "evaluate_log_loss(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c9bd26-a04e-4383-b97e-fd9f45edbb0d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Spellschool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5b79d59a-dccf-4034-ba66-d6d49c1c886c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(DATA_PATH_HEARTHSTONE_SPELLSCHOOL + \"/train.csv\")\n",
    "df_test = pd.read_csv(DATA_PATH_HEARTHSTONE_SPELLSCHOOL + \"/test.csv\")\n",
    "df_dev = pd.read_csv(DATA_PATH_HEARTHSTONE_SPELLSCHOOL + \"/dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "217cd644-e509-4b3b-aca0-855037b7fc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6dd99155-8362-45ab-bd8a-4f507924f916",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2715/2715 [02:43<00:00, 16.65it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 508/508 [00:30<00:00, 16.70it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 170/170 [00:10<00:00, 16.39it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = process_df(df_train, DATA_PATH_HEARTHSTONE_SPELLSCHOOL, clip_model, clip_preprocess, True)\n",
    "train_labels = label_encoder.fit_transform(df_train['spellSchool'])\n",
    "test_data = process_df(df_test, DATA_PATH_HEARTHSTONE_SPELLSCHOOL, clip_model, clip_preprocess, True)\n",
    "test_labels = label_encoder.fit_transform(df_test['spellSchool'])\n",
    "dev_data = process_df(df_dev, DATA_PATH_HEARTHSTONE_SPELLSCHOOL, clip_model, clip_preprocess, True)\n",
    "dev_labels = label_encoder.fit_transform(df_dev['spellSchool'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1e81f902-8824-466f-9d50-7ffe68edfda6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ARCANE': 0,\n",
       " 'FEL': 1,\n",
       " 'FIRE': 2,\n",
       " 'FROST': 3,\n",
       " 'HOLY': 4,\n",
       " 'NATURE': 5,\n",
       " 'NONE_spellSchool': 6,\n",
       " 'SHADOW': 7}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "class_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "01bf163e-d742-4822-a8c5-8711fc0fc077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
       "       29, 29, 29, 29, 29, 29,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 30, 30, 30, 30, 30,\n",
       "       30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30])"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ddb52b18-bb3c-4133-ad24-75c86f73e520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialized model\n",
    "input_size = train_data[0].size(1)\n",
    "hidden_size = 128\n",
    "num_epochs = 20\n",
    "num_classes = len(label_encoder.classes_)\n",
    "lr = 0.0002\n",
    "batch_size = 16\n",
    "    \n",
    "model = Model(input_size, hidden_size, num_classes).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "86448062-1f77-45d3-9cb6-75a531dee330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize data loader\n",
    "train_dataset = CustomDataset(train_data, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = CustomDataset(test_data, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "dev_dataset = CustomDataset(dev_data, dev_labels)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "93a33927-255d-4cbc-ab19-39afad9c301b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20]\n",
      "  Train Loss: 1.3241 - Validation Loss: 1.1467\n",
      "Epoch [2/20]\n",
      "  Train Loss: 1.1155 - Validation Loss: 1.0641\n",
      "Epoch [3/20]\n",
      "  Train Loss: 1.0132 - Validation Loss: 0.9729\n",
      "Epoch [4/20]\n",
      "  Train Loss: 0.9169 - Validation Loss: 0.8972\n",
      "Epoch [5/20]\n",
      "  Train Loss: 0.8298 - Validation Loss: 0.8168\n",
      "Epoch [6/20]\n",
      "  Train Loss: 0.7530 - Validation Loss: 0.8205\n",
      "Epoch [7/20]\n",
      "  Train Loss: 0.6885 - Validation Loss: 0.7211\n",
      "Epoch [8/20]\n",
      "  Train Loss: 0.6185 - Validation Loss: 0.6820\n",
      "Epoch [9/20]\n",
      "  Train Loss: 0.5871 - Validation Loss: 0.6805\n",
      "Epoch [10/20]\n",
      "  Train Loss: 0.5267 - Validation Loss: 0.6220\n",
      "Epoch [11/20]\n",
      "  Train Loss: 0.4939 - Validation Loss: 0.6016\n",
      "Epoch [12/20]\n",
      "  Train Loss: 0.4635 - Validation Loss: 0.5671\n",
      "Epoch [13/20]\n",
      "  Train Loss: 0.4389 - Validation Loss: 0.5519\n",
      "Epoch [14/20]\n",
      "  Train Loss: 0.4091 - Validation Loss: 0.5563\n",
      "Epoch [15/20]\n",
      "  Train Loss: 0.3669 - Validation Loss: 0.5244\n",
      "Epoch [16/20]\n",
      "  Train Loss: 0.3503 - Validation Loss: 0.5503\n",
      "Epoch [17/20]\n",
      "  Train Loss: 0.3324 - Validation Loss: 0.5291\n",
      "Epoch [18/20]\n",
      "  Train Loss: 0.3093 - Validation Loss: 0.5387\n",
      "Epoch [19/20]\n",
      "  Train Loss: 0.2841 - Validation Loss: 0.5218\n",
      "Epoch [20/20]\n",
      "  Train Loss: 0.2640 - Validation Loss: 0.5198\n",
      "Accuracy: 0.841\n",
      "Log Loss: 0.499\n"
     ]
    }
   ],
   "source": [
    "# train model and evaluation\n",
    "train(model, train_loader, dev_loader)\n",
    "evaluate_accuracy(model, test_loader)\n",
    "evaluate_log_loss(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9e1c83-517c-41db-86ea-bcad52f5b145",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "736f6a31-c6e2-4a74-80dd-aaa25dd45d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "95d70a15-1993-472b-9c8c-3d7225a1def0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = TabularDataset(f'{DATA_PATH_HEARTHSTONE_RACE}/train.csv')\n",
    "label = 'race'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9e1b1416-a8a4-4790-bbe3-27c89b939b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20240429_150521\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20240429_150521\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.11.9\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "CPU Count:          12\n",
      "Memory Avail:       22.86 GB / 31.95 GB (71.5%)\n",
      "Disk Space Avail:   226.86 GB / 465.13 GB (48.8%)\n",
      "===================================================\n",
      "Train Data Rows:    5398\n",
      "Train Data Columns: 13\n",
      "Label Column:       race\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == object).\n",
      "\tFirst 10 (of 15) unique label values:  ['NONE_race', 'BEAST', 'DRAGON', 'DEMON', 'PIRATE', 'TOTEM', 'MURLOC', 'ELEMENTAL', 'MECHANICAL', 'QUILBOAR']\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 15\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    23409.58 MB\n",
      "\tTrain Data (Original)  Memory Usage: 3.50 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['text']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 336\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 2): ['id', 'Image Path']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('object', []) : 2 | ['id', 'Image Path']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])        : 1 | ['collectible']\n",
      "\t\t('int', [])          : 3 | ['health', 'attack', 'cost']\n",
      "\t\t('object', [])       : 6 | ['cardClass', 'name', 'set', 'rarity', 'artist', ...]\n",
      "\t\t('object', ['text']) : 1 | ['text']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :   6 | ['cardClass', 'name', 'set', 'rarity', 'artist', ...]\n",
      "\t\t('category', ['text_as_category'])  :   1 | ['text']\n",
      "\t\t('int', [])                         :   3 | ['health', 'attack', 'cost']\n",
      "\t\t('int', ['binned', 'text_special']) :  18 | ['text.char_count', 'text.word_count', 'text.capital_ratio', 'text.lower_ratio', 'text.digit_ratio', ...]\n",
      "\t\t('int', ['bool'])                   :   1 | ['collectible']\n",
      "\t\t('int', ['text_ngram'])             : 317 | ['__nlp__.10', '__nlp__.add', '__nlp__.add random', '__nlp__.adjacent', '__nlp__.adjacent minions', ...]\n",
      "\t5.4s = Fit runtime\n",
      "\t11 features in original data used to generate 346 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 3.55 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 5.5s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 4858, Val Rows: 540\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "C:\\Users\\Felix\\anaconda3\\envs\\masterthesis\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] Das System kann die angegebene Datei nicht finden\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\masterthesis\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\masterthesis\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\masterthesis\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\masterthesis\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\t0.5315\t = Validation score   (accuracy)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.6259\t = Validation score   (accuracy)\n",
      "\t0.06s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t0.7852\t = Validation score   (accuracy)\n",
      "\t6.06s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\t0.7648\t = Validation score   (accuracy)\n",
      "\t9.46s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t0.7648\t = Validation score   (accuracy)\n",
      "\t11.53s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.7315\t = Validation score   (accuracy)\n",
      "\t1.24s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.7222\t = Validation score   (accuracy)\n",
      "\t1.17s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t0.7889\t = Validation score   (accuracy)\n",
      "\t363.98s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.7296\t = Validation score   (accuracy)\n",
      "\t1.11s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.7222\t = Validation score   (accuracy)\n",
      "\t1.14s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.7574\t = Validation score   (accuracy)\n",
      "\t11.84s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.6593\t = Validation score   (accuracy)\n",
      "\t20.83s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t0.7426\t = Validation score   (accuracy)\n",
      "\t18.86s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'CatBoost': 0.364, 'NeuralNetFastAI': 0.273, 'LightGBMXT': 0.091, 'LightGBM': 0.091, 'RandomForestGini': 0.091, 'LightGBMLarge': 0.091}\n",
      "\t0.8259\t = Validation score   (accuracy)\n",
      "\t0.16s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 455.93s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20240429_150521\")\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(label=label).fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b597ac54-4352-4113-96b6-c756ebb6e008",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predictor.predict(df_test.drop(columns=[label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cf8326cf-e388-49b5-ac5b-ae85e916fac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7766798418972332,\n",
       " 'balanced_accuracy': 0.6358084823002375,\n",
       " 'mcc': 0.6556862418092518}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.evaluate(df_test, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de936555-5611-4b52-a97c-0a84a6d3852d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterthesis",
   "language": "python",
   "name": "masterthesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
